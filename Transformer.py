# -*- coding: utf-8 -*-
"""21HS3AI27_assignment_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14DIe8pt619zGo0AAy-jgFBrKglY2thxx
"""

!pip -q install portalocker
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install -q torchtext==0.15.2
!pip install -q torchinfo
!pip install torch==2.1.0 torchtext==0.16.0
!pip install --upgrade torchvision torchaudio

!pip uninstall -y torchtext
!pip install -q torchtext==0.16.0 --index-url https://download.pytorch.org/whl/cu118

!pip install datasets

!pip install -q transformers datasets

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from datasets import load_dataset
from transformers import BertTokenizer
import numpy as np
import math
import string
import nltk
from nltk.corpus import stopwords
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
from collections import Counter
from tqdm.notebook import tqdm

import torch
from datasets import load_dataset
from torch.utils.data import DataLoader, TensorDataset, random_split
from transformers import BertTokenizer
from torchtext.vocab import build_vocab_from_iterator
import nltk
from nltk.corpus import stopwords
import string
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt
import math
import torch.nn as nn
import torch.nn.functional as F

"""# Task 1 - Loading SST2 dataset from HuggingFace

"""

nltk.download('stopwords')

print("Loading SST2 dataset...")
dataset = load_dataset("sst2")
train_data = dataset['train']
valid_data = dataset['validation']

print("\nSample data from SST2 dataset:")
for i in range(5):
    print(f"Text: {train_data[i]['sentence']}")
    print(f"Label: {train_data[i]['label']}")
    print()

from transformers import BertTokenizer

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Getting stopwords and punctuations
stop_words = set(stopwords.words('english'))
punctuations = set(string.punctuation)

def preprocess_text(text):
    # Tokenizing with BERT tokenizer
    tokens = bert_tokenizer.tokenize(text)

    # Removing stopwords and punctuations
    tokens = [token for token in tokens
              if token.lower() not in stop_words
              and token not in punctuations]

    return tokens

# Preprocessing all the texts
train_texts = [preprocess_text(example['sentence']) for example in train_data]
test_texts = [preprocess_text(example['sentence']) for example in valid_data]

"""Building vocabulary"""

vocab = build_vocab_from_iterator(train_texts, specials=['<unk>', '<pad>'])
vocab.set_default_index(vocab['<unk>'])

# Numericalizing texts
train_numerical = [vocab(tokens) for tokens in train_texts]
test_numerical = [vocab(tokens) for tokens in test_texts]

train_labels = torch.tensor([example['label'] for example in train_data], dtype=torch.long)
test_labels = torch.tensor([example['label'] for example in valid_data], dtype=torch.long)

"""# Task 2 - Split train into train and validation (80-20)"""

max_len = 64

#Padding function below
def pad_sequence(sequences, max_len=max_len):
    padded = torch.zeros((len(sequences), max_len), dtype=torch.long)
    for i, seq in enumerate(sequences):
        length = min(len(seq), max_len)
        padded[i, :length] = torch.tensor(seq[:length], dtype=torch.long)
    return padded

#applying padding function
train_padded = pad_sequence(train_numerical)
test_padded = pad_sequence(test_numerical)

train_dataset = TensorDataset(train_padded, train_labels)
test_dataset = TensorDataset(test_padded, test_labels)

# Splitting train into train and validation
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

train_labels = [label.item() for _, label in train_dataset]
label_counts = {}
for label in train_labels:
    label_counts[label] = label_counts.get(label, 0) + 1

print("\nLabel distribution in training set:")
total_samples = len(train_labels)
for label, count in label_counts.items():
    percentage = (count / total_samples) * 100
    print(f"Label {label}: {count} samples ({percentage:.2f}%)")

"""# Task 3 - Custom Transformer Model"""

import torch
import torch.nn as nn
import math
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

# Here is the Transformer Encoder Layer
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dropout=0.5):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead)
        self.linear = nn.Linear(d_model, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn_out, _ = self.self_attn(x, x, x)
        x = self.norm1(x + self.dropout(attn_out))
        ff = self.linear(x)
        return self.norm2(x + self.dropout(ff))

# Positional Encoding function
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))  # order dim are (1, max_len, d_model)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

# Here is the Transformer Model for Classification
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, num_classes, d_model=16, nhead=4, num_layers=1, dropout=0.5):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model)
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, nhead, dropout=dropout) for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(d_model, num_classes)
        self.d_model = d_model

    def forward(self, x):
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)
        x = x.transpose(0, 1)  # order is (seq_len, batch_size, d_model)
        for layer in self.encoder_layers:
            x = layer(x)
        x = x.mean(dim=0)
        x = self.dropout(x)
        return self.fc(x)

import torch
import torch.nn as nn

def train_model(model, train_loader, val_loader, num_epochs=5, lr=1e-3):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_train_loss = 0.0
        for texts, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(texts)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_train_loss += loss.item()

        average_train_loss = running_train_loss / len(train_loader)
        train_losses.append(average_train_loss)

        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for texts, labels in val_loader:
                outputs = model(texts)
                loss = criterion(outputs, labels)
                running_val_loss += loss.item()

        average_val_loss = running_val_loss / len(val_loader)
        val_losses.append(average_val_loss)

        print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Val Loss: {average_val_loss:.4f}")

    return train_losses, val_losses


# Here is the Evaluation Function
def evaluate_model(model, loader):
    model.eval()
    all_preds, all_labels = [], [] # empty lists initiated

    with torch.no_grad():
        for texts, labels in loader:
            outputs = model(texts)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels.cpu().tolist()) #using tolist as numpy was generating error

    correct = sum(p == l for p, l in zip(all_preds, all_labels))
    accuracy = correct / len(all_labels) if all_labels else 0.0
    true_pos = sum((p == 1 and l == 1) for p, l in zip(all_preds, all_labels))
    predicted_pos = sum(p == 1 for p in all_preds)
    actual_pos = sum(l == 1 for l in all_labels)

    precision = true_pos / predicted_pos if predicted_pos > 0 else 0.0
    recall = true_pos / actual_pos if actual_pos > 0 else 0.0

    return accuracy, precision, recall #returning the three metrics

# Initializing the model
num_classes=2
vocab_size = len(vocab)
model = TransformerModel(vocab_size,num_classes)

# Creating the DataLoaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Training the first model
print("\nTraining the first custom transformer model...")
train_losses, val_losses = train_model(model, train_loader, val_loader)

# Plotting the graphs of losses
plt.figure()
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Losses (Custom Transformer)')
plt.legend()

accuracy, precision, recall = evaluate_model(model, test_loader)



accuracy

precision

recall

"""# Model summary"""

print(model)

"""# Task 4"""

# Task 4 -
# Built-in Transformer Encoder Layer
class BuiltInTransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model=16, nhead=4, num_layers=1, dropout=0.5):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.dropout = nn.Dropout(dropout)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_model, dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        self.fc = nn.Linear(d_model, 2)

    def forward(self, src):
        src = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)
        src = self.pos_encoder(src)
        src = self.dropout(src)
        src = src.transpose(0, 1)  # dimensions are (seq_len, batch_size, d_model)
        output = self.transformer_encoder(src)
        output = output.mean(dim=0)
        output = self.fc(output)
        return output

# Initializing the task 4 model
builtin_model = BuiltInTransformerModel(vocab_size)

# Training the model
print("\nTraining built-in transformer model...")
builtin_train_losses, builtin_val_losses = train_model(builtin_model, train_loader, val_loader)

# Plotting the graph of losses
plt.figure()
plt.plot(builtin_train_losses, label='Training Loss')
plt.plot(builtin_val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Losses (Built-in Transformer)')
plt.legend()

# Evaluating on test set
builtin_test_accuracy, builtin_test_precision, builtin_test_recall = evaluate_model(builtin_model, test_loader)
print(f"\nTest Performance (Built-in Transformer):")
print(f"Accuracy: {builtin_test_accuracy:.4f}")
print(f"Precision: {builtin_test_precision:.4f}")
print(f"Recall: {builtin_test_recall:.4f}")

# Evaluating on the test set
builtin_test_accuracy, builtin_test_precision, builtin_test_recall = evaluate_model(builtin_model, test_loader)
print(f"\nTest Performance (Built-in Transformer):")
print(f"Accuracy: {builtin_test_accuracy:.4f}")
print(f"Precision: {builtin_test_precision:.4f}")
print(f"Recall: {builtin_test_recall:.4f}")

print(builtin_model)

"""# Task 5"""

!pip install transformers torch
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers torch

!pip uninstall -y torchvision torchaudio
!pip install -q torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

!pip install -q huggingface_hub transformers

import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score

from transformers import BertModel

from huggingface_hub import notebook_login

notebook_login()

from datasets import load_dataset

dataset = load_dataset("glue", "sst2")

!pip install -q spacy
!python -m spacy download en_core_web_sm
import spacy

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast, BertModel
from collections import Counter
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
from datasets import load_dataset

# Device configuation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

from datasets import load_dataset

# Loading the SST-2 dataset
dataset = load_dataset("glue", "sst2")
train_data = dataset['train']
test_data = dataset['validation']

# Splitting train_data into 80% train and 20% validation
train_val_split = train_data.train_test_split(test_size=0.2, seed=42)
train_data = train_val_split['train']
valid_data = train_val_split['test']

print(f"Train size: {len(train_data)}")
print(f"Validation size: {len(valid_data)}")
print(f"Test size: {len(test_data)}")

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
label_counter = Counter(example['label'] for example in train_data)
label2id = {label: i for i, label in enumerate(label_counter)}
id2label = {i: label for label, i in label2id.items()}

MAX_LEN = 64

def encode_sst(example):
    enc = tokenizer(example['sentence'],
                   truncation=True,
                   max_length=MAX_LEN,
                   padding='max_length',
                   return_tensors="pt")
    return enc["input_ids"].squeeze(), enc["attention_mask"].squeeze(), torch.tensor(example['label'])

def encode_dataset(data):
    input_ids, attention_masks, labels = [], [], []
    for example in data:
        ids, mask, label = encode_sst(example)
        input_ids.append(ids)
        attention_masks.append(mask)
        labels.append(label)
    return torch.stack(input_ids), torch.stack(attention_masks), torch.stack(labels)

X_train, M_train, Y_train = encode_dataset(train_data)
X_valid, M_valid, Y_valid = encode_dataset(valid_data)
X_test, M_test, Y_test = encode_dataset(test_data)

class SSTDataset(Dataset):
    def __init__(self, X, M, Y):
        self.X = X
        self.M = M
        self.Y = Y
    def __len__(self): return len(self.X)
    def __getitem__(self, i): return self.X[i], self.M[i], self.Y[i]

# Creating DataLoaders with batch size 32
batch_size = 32
train_dl = DataLoader(SSTDataset(X_train, M_train, Y_train), batch_size=batch_size, shuffle=False)
valid_dl = DataLoader(SSTDataset(X_valid, M_valid, Y_valid), batch_size=batch_size)
test_dl = DataLoader(SSTDataset(X_test, M_test, Y_test), batch_size=batch_size)

# bert model for our dataset
class BERT_SST(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        # Freezing the bert parameters
        for param in self.bert.parameters():
            param.requires_grad = False
        self.classifier = nn.Linear(self.bert.config.hidden_size, len(label2id))  # have 2 classes

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # Using the pooled output for sentence classification
        return self.classifier(outputs.pooler_output)

# Initializing the model, loss and the adam optimizer
model = BERT_SST().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# Training loop
train_losses, val_losses = [], [] #empty lists

for epoch in range(5): #using 5 epochs as given in assignment
    start_time = time.time()
    model.train()
    total_loss = 0

    loop = tqdm(train_dl, desc=f"Epoch {epoch+1}")
    for xb, mask, yb in loop:
        xb, mask, yb = xb.to(device), mask.to(device), yb.to(device)

        optimizer.zero_grad()
        out = model(xb, mask)
        loss = criterion(out, yb)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())

    train_losses.append(total_loss / len(train_dl))

    # Validation loss append
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for xb, mask, yb in valid_dl:
            xb, mask, yb = xb.to(device), mask.to(device), yb.to(device)
            out = model(xb, mask)
            val_loss += criterion(out, yb).item()

    val_losses.append(val_loss / len(valid_dl))
    print(f"Epoch {epoch+1} finished in {time.time() - start_time:.2f} seconds")

# Plotting the graph of losses
plt.figure(figsize=(8, 5))
plt.plot(train_losses, label='Training Loss', marker='o')
plt.plot(val_losses, label='Validation Loss', marker='s')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title(' Training vs Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

#evaluation function below
def evaluate(model, loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for xb, mask, yb in loader:
            xb, mask, yb = xb.to(device), mask.to(device), yb.to(device)
            out = model(xb, mask)
            _, preds = torch.max(out, 1)
            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(yb.cpu().tolist())

    return {
        'accuracy': accuracy_score(all_labels, all_preds),
        'precision': precision_score(all_labels, all_preds, average='weighted'),
        'recall': recall_score(all_labels, all_preds, average='weighted'),
        'f1': f1_score(all_labels, all_preds, average='weighted')
    }

# Evaluating on test set
test_metrics = evaluate(model, test_dl)
print("\nTest Metrics:")
print(f"Accuracy: {test_metrics['accuracy']:.4f}")
print(f"Precision: {test_metrics['precision']:.4f}")
print(f"Recall: {test_metrics['recall']:.4f}")
print(f"F1 Score: {test_metrics['f1']:.4f}")

print(model)

!pip install torchinfo

from torchinfo import summary

"""# Summary of Bert Model used Task-5"""

from torchinfo import summary
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
dummy_input_ids = torch.randint(0, 100, (32, 64)).to(device)
dummy_attention_mask = torch.ones((32, 64)).to(device)
summary(model, input_data=(dummy_input_ids, dummy_attention_mask))

"""# Summary of Built-in Transformer Model Task - 4"""

from torchinfo import summary
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
builtin_model.to(device)

dummy_input = torch.randint(0, vocab_size, (32, 64)).to(device)


summary(builtin_model, input_data=dummy_input)

"""# Torch Summary for Task -3"""

print(model)

from torchinfo import summary
import torch

vocab_size = 11419
num_classes = 2
seq_len = 64
batch_size = 32

model = TransformerModel(vocab_size, num_classes)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)

summary(model, input_data=dummy_input)

"""#Insights

- **Custom Transformer**: A simple transformer architecture with moderate performance.
- **nn.TransformerEncoderLayer**: using PyTorch’s optimized encoder improves generalization and learning efficiency compared to custom implementation.
- **BERT Transfer Learning**: Offers the best results across all metrics. The pretrained BERT encoder captures deep contextual understanding, even when fine-tuning only a linear layer.
- **Training Curve**: Loss curves indicate smooth convergence without overfitting for BERT.

# RESULTs
"""

results = {
    "Task 3 - Custom Transformer Encoder": {
        "Model Type": "Custom Transformer",
        "Accuracy": 0.7328,
        "Precision": 0.7414,
        "Recall": 0.7297,
        "F1 Score": None
    },
    "Task 4 - Built-in TransformerEncoderLayer": {
        "Model Type": "nn.TransformerEncoderLayer",
        "Accuracy": 0.7339,
        "Precision": 0.7789,
        "Recall": 0.6667,
        "F1 Score": None
    },
    "Task 5 - Transfer Learning with BERT": {
        "Model Type": "BERT + Linear Layer",
        "Accuracy": 0.8188,
        "Precision": 0.8192,
        "Recall": 0.8188,
        "F1 Score": 0.8187
    }
}

# Print the results in a readable format
for task, metrics in results.items():
    print(f"\n{task}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value}")